{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers[torch] datasets[audio] accelerate evaluate jiwer librosa torchcodec"
      ],
      "metadata": {
        "id": "PyXQzK6U313N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8a5b73-7586-4dd7-ae6b-661b3a4ca597"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load G drive and change directory"
      ],
      "metadata": {
        "id": "h-zfA5IKGBDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMqlWOa6F_7y",
        "outputId": "2759d165-d294-451c-e227-38aec1c9ca04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/sinhala_pronunciation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFD8mA8CGUmT",
        "outputId": "04ecdd0f-d498-4edf-ec02-f6e8c7d37a92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sinhala_pronunciation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loader"
      ],
      "metadata": {
        "id": "wSydLqEvHUFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_loader.py\n",
        "import os, json, torch, torchaudio\n",
        "\n",
        "root = \"/content/drive/MyDrive/sinhala_pronunciation/dataset\"\n",
        "\n",
        "import os, json, torch, torchaudio\n",
        "\n",
        "class SinhalaDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.samples = []\n",
        "        with open(f\"{root}/phonemes.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data_map = json.load(f)\n",
        "\n",
        "        for folder_name, info in self.data_map.items():\n",
        "            folder_path = os.path.join(root, folder_name)\n",
        "\n",
        "            if os.path.exists(folder_path):\n",
        "                for f in os.listdir(folder_path):\n",
        "                    if f.endswith(\".wav\"):\n",
        "                        # Store the path AND the specific phoneme list\n",
        "                        self.samples.append((os.path.join(folder_path, f), info[\"phonemes\"]))\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Missing folder: {folder_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path, phonemes = self.samples[idx]\n",
        "        wav, sr = torchaudio.load(wav_path)\n",
        "        if sr != 16000:\n",
        "            wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "\n",
        "        return wav.squeeze(), phonemes\n"
      ],
      "metadata": {
        "id": "SrFprIFY4BNt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data loading"
      ],
      "metadata": {
        "id": "mMpaKdegG4in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SinhalaDataset(root)\n",
        "print(len(dataset))\n",
        "\n",
        "wav, phones = dataset[0]\n",
        "print(wav.shape)\n",
        "print(phones)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWy7sBKXG5xM",
        "outputId": "ea3a8e90-1849-4fbb-bf60-2b83461968b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1618\n",
            "torch.Size([10664])\n",
            "['b', 'a', 'l', 'l', 'a:']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating vocabulary with mappings for silence, etc"
      ],
      "metadata": {
        "id": "gyBswqFkLoE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# existing vocab\n",
        "existing_vocab = {\n",
        "    \"a\": 0, \"a:\": 1, \"b\": 2, \"l\": 3, \"i\": 4, \"d\": 5, \"th\": 6,\n",
        "    \"p\": 7, \"o\": 8, \"k\": 9, \"s\": 10, \"m\": 11, \"n\": 12, \"y\": 13,\n",
        "    \"u\": 14, \"r\": 15, \"w\": 16, \"h\": 17, \"g\": 18, \"i:\": 19, \"sh\": 20\n",
        "}\n",
        "\n",
        "training_vocab = existing_vocab.copy()\n",
        "training_vocab[\"[PAD]\"] = 21\n",
        "training_vocab[\"[UNK]\"] = 22\n",
        "# training_vocab[\"[BLANK]\"] = 23 # separate sounds\n",
        "\n",
        "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_vocab, f)\n",
        "\n",
        "print(f\"‚úÖ Created training vocab with {len(training_vocab)} tokens.\")"
      ],
      "metadata": {
        "id": "ZzMuJ8X-G8Es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462c8b66-1397-4dd7-b67d-eb06f51947e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created training vocab with 23 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training execution"
      ],
      "metadata": {
        "id": "8_Y9y0nPMx9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "# --- 1. SET DEVICE ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 2. LOAD VOCAB & PROCESSOR ---\n",
        "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    training_vocab = json.load(f)\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    \"./vocab.json\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    word_delimiter_token=None\n",
        ")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\n",
        "    feature_size=1, sampling_rate=16000, padding_value=0.0,\n",
        "    do_normalize=True, return_attention_mask=True\n",
        ")\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "from transformers import Wav2Vec2Config\n",
        "\n",
        "# --- 3. LOAD MODEL ---\n",
        "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n",
        "\n",
        "# 2. Update config with specific parameters\n",
        "config.ctc_loss_reduction = \"mean\"\n",
        "config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "config.vocab_size = len(processor.tokenizer)\n",
        "# config.ctc_blank_id = training_vocab.get(\"[BLANK]\", 0)\n",
        "\n",
        "# 3. Load the model using this config\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-xls-r-300m\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True # Required because, changed vocab_size\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "model.freeze_feature_encoder()\n",
        "\n",
        "# --- 4. DATA COLLATOR ---\n",
        "def collate_fn(batch):\n",
        "    # Separate the waveforms and the phoneme lists\n",
        "    audio_list = [item[0].numpy() for item in batch]\n",
        "    phoneme_lists = [item[1] for item in batch]\n",
        "\n",
        "    # 1. Process Audio (Input)\n",
        "    inputs = processor(\n",
        "        audio_list,\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # 2. Process Labels (Target) using the tokenizer directly\n",
        "    # This avoids the 'multiple values for padding' error\n",
        "    labels_batch = processor.tokenizer(\n",
        "        phoneme_lists,\n",
        "        is_split_into_words=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Replace padding index with -100 so CTC loss ignores it\n",
        "    labels = labels_batch.input_ids.masked_fill(\n",
        "        labels_batch.input_ids == processor.tokenizer.pad_token_id, -100\n",
        "    )\n",
        "\n",
        "    return inputs.input_values.to(device), labels.to(device)\n",
        "\n",
        "# --- 5. STABILIZED TRAINING LOOP ---\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# 1. Update Config to handle infinite loss\n",
        "model.config.ctc_zero_infinity = True\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "model.train()\n",
        "print(\"üöÄ Starting stabilized training...\")\n",
        "\n",
        "for epoch in range(20): # used 15, 18\n",
        "\n",
        "    if epoch == 5:\n",
        "      model.unfreeze_feature_encoder()\n",
        "      print(\"üîì Feature encoder unfrozen\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_values=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # 2. Check for NaN before backprop\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"‚ö†Ô∏è Skip NaN batch {batch_idx}\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 3. Gradient Clipping (Crucial for stability)\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = epoch_loss / len(loader)\n",
        "    print(f\"‚úÖ Epoch {epoch} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# --- 6. SAVE ---\n",
        "model.save_pretrained(\"sinhala-pronunciation-model\")\n",
        "processor.save_pretrained(\"sinhala-pronunciation-model\")\n",
        "print(\"üíæ Done! Model saved to 'sinhala-pronunciation-model' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gj_XMahMzBA",
        "outputId": "d41477e3-53e5-4628-c675-2f5368e955d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting stabilized training...\n",
            "Batch 0 | Loss: 5.1879\n",
            "Batch 10 | Loss: 3.3018\n",
            "Batch 20 | Loss: 3.6023\n",
            "Batch 30 | Loss: 2.6611\n",
            "Batch 40 | Loss: 1.9498\n",
            "Batch 50 | Loss: 1.7895\n",
            "Batch 60 | Loss: 1.7031\n",
            "Batch 70 | Loss: 1.7950\n",
            "Batch 80 | Loss: 2.2586\n",
            "Batch 90 | Loss: 1.8603\n",
            "Batch 100 | Loss: 2.3659\n",
            "Batch 110 | Loss: 1.7227\n",
            "Batch 120 | Loss: 1.7723\n",
            "Batch 130 | Loss: 1.7000\n",
            "Batch 140 | Loss: 1.6639\n",
            "Batch 150 | Loss: 2.4130\n",
            "Batch 160 | Loss: 1.6893\n",
            "Batch 170 | Loss: 1.6230\n",
            "Batch 180 | Loss: 2.0014\n",
            "Batch 190 | Loss: 1.6639\n",
            "Batch 200 | Loss: 1.4880\n",
            "Batch 210 | Loss: 2.0273\n",
            "Batch 220 | Loss: 1.8302\n",
            "Batch 230 | Loss: 1.9126\n",
            "Batch 240 | Loss: 1.7736\n",
            "Batch 250 | Loss: 1.5837\n",
            "Batch 260 | Loss: 1.7097\n",
            "Batch 270 | Loss: 1.5888\n",
            "Batch 280 | Loss: 1.9160\n",
            "Batch 290 | Loss: 1.6132\n",
            "Batch 300 | Loss: 1.6456\n",
            "Batch 310 | Loss: 1.6714\n",
            "Batch 320 | Loss: 1.8384\n",
            "Batch 330 | Loss: 1.6013\n",
            "Batch 340 | Loss: 2.0988\n",
            "Batch 350 | Loss: 1.6631\n",
            "Batch 360 | Loss: 1.6865\n",
            "Batch 370 | Loss: 1.8957\n",
            "Batch 380 | Loss: 1.8457\n",
            "Batch 390 | Loss: 1.6521\n",
            "Batch 400 | Loss: 1.5736\n",
            "‚úÖ Epoch 0 | Avg Loss: 1.8862\n",
            "Batch 0 | Loss: 1.6262\n",
            "Batch 10 | Loss: 1.7233\n",
            "Batch 20 | Loss: 1.6392\n",
            "Batch 30 | Loss: 1.6196\n",
            "Batch 40 | Loss: 1.4605\n",
            "Batch 50 | Loss: 1.5514\n",
            "Batch 60 | Loss: 1.6019\n",
            "Batch 70 | Loss: 1.5157\n",
            "Batch 80 | Loss: 1.3655\n",
            "Batch 90 | Loss: 1.2253\n",
            "Batch 100 | Loss: 1.3116\n",
            "Batch 110 | Loss: 1.0176\n",
            "Batch 120 | Loss: 1.4224\n",
            "Batch 130 | Loss: 1.5974\n",
            "Batch 140 | Loss: 1.3034\n",
            "Batch 150 | Loss: 1.1648\n",
            "Batch 160 | Loss: 1.5211\n",
            "Batch 170 | Loss: 1.1095\n",
            "Batch 180 | Loss: 1.3543\n",
            "Batch 190 | Loss: 1.2203\n",
            "Batch 200 | Loss: 1.2090\n",
            "Batch 210 | Loss: 1.2961\n",
            "Batch 220 | Loss: 1.2869\n",
            "Batch 230 | Loss: 1.3215\n",
            "Batch 240 | Loss: 0.8744\n",
            "Batch 250 | Loss: 0.9520\n",
            "Batch 260 | Loss: 1.1038\n",
            "Batch 270 | Loss: 0.9350\n",
            "Batch 280 | Loss: 1.3123\n",
            "Batch 290 | Loss: 0.9606\n",
            "Batch 300 | Loss: 1.2483\n",
            "Batch 310 | Loss: 1.0823\n",
            "Batch 320 | Loss: 1.3301\n",
            "Batch 330 | Loss: 1.2846\n",
            "Batch 340 | Loss: 1.0560\n",
            "Batch 350 | Loss: 1.2798\n",
            "Batch 360 | Loss: 1.3486\n",
            "Batch 370 | Loss: 1.1169\n",
            "Batch 380 | Loss: 1.1417\n",
            "Batch 390 | Loss: 1.3448\n",
            "Batch 400 | Loss: 1.6397\n",
            "‚úÖ Epoch 1 | Avg Loss: 1.2977\n",
            "Batch 0 | Loss: 1.1412\n",
            "Batch 10 | Loss: 1.1810\n",
            "Batch 20 | Loss: 0.9815\n",
            "Batch 30 | Loss: 0.9494\n",
            "Batch 40 | Loss: 1.0534\n",
            "Batch 50 | Loss: 0.8546\n",
            "Batch 60 | Loss: 1.1086\n",
            "Batch 70 | Loss: 1.0247\n",
            "Batch 80 | Loss: 0.8286\n",
            "Batch 90 | Loss: 1.2248\n",
            "Batch 100 | Loss: 0.9285\n",
            "Batch 110 | Loss: 1.1723\n",
            "Batch 120 | Loss: 1.0471\n",
            "Batch 130 | Loss: 0.7987\n",
            "Batch 140 | Loss: 1.3220\n",
            "Batch 150 | Loss: 1.0501\n",
            "Batch 160 | Loss: 1.0501\n",
            "Batch 170 | Loss: 0.8923\n",
            "Batch 180 | Loss: 1.1581\n",
            "Batch 190 | Loss: 0.8683\n",
            "Batch 200 | Loss: 1.3361\n",
            "Batch 210 | Loss: 0.8323\n",
            "Batch 220 | Loss: 0.8032\n",
            "Batch 230 | Loss: 0.8335\n",
            "Batch 240 | Loss: 0.8898\n",
            "Batch 250 | Loss: 1.1212\n",
            "Batch 260 | Loss: 0.9128\n",
            "Batch 270 | Loss: 1.0239\n",
            "Batch 280 | Loss: 0.8326\n",
            "Batch 290 | Loss: 0.8770\n",
            "Batch 300 | Loss: 1.0001\n",
            "Batch 310 | Loss: 1.1221\n",
            "Batch 320 | Loss: 0.9704\n",
            "Batch 330 | Loss: 1.1316\n",
            "Batch 340 | Loss: 0.8854\n",
            "Batch 350 | Loss: 0.8565\n",
            "Batch 360 | Loss: 0.8242\n",
            "Batch 370 | Loss: 0.9664\n",
            "Batch 380 | Loss: 0.8216\n",
            "Batch 390 | Loss: 0.9091\n",
            "Batch 400 | Loss: 0.8004\n",
            "‚úÖ Epoch 2 | Avg Loss: 1.0225\n",
            "Batch 0 | Loss: 0.9094\n",
            "Batch 10 | Loss: 0.7272\n",
            "Batch 20 | Loss: 0.8434\n",
            "Batch 30 | Loss: 0.7781\n",
            "Batch 40 | Loss: 0.7685\n",
            "Batch 50 | Loss: 1.0947\n",
            "Batch 60 | Loss: 0.9114\n",
            "Batch 70 | Loss: 0.7668\n",
            "Batch 80 | Loss: 0.9520\n",
            "Batch 90 | Loss: 0.7022\n",
            "Batch 100 | Loss: 0.9440\n",
            "Batch 110 | Loss: 0.6917\n",
            "Batch 120 | Loss: 0.7904\n",
            "Batch 130 | Loss: 0.9950\n",
            "Batch 140 | Loss: 0.8219\n",
            "Batch 150 | Loss: 0.6393\n",
            "Batch 160 | Loss: 0.9237\n",
            "Batch 170 | Loss: 0.7177\n",
            "Batch 180 | Loss: 0.7302\n",
            "Batch 190 | Loss: 0.6859\n",
            "Batch 200 | Loss: 0.6536\n",
            "Batch 210 | Loss: 0.8371\n",
            "Batch 220 | Loss: 0.7822\n",
            "Batch 230 | Loss: 0.9325\n",
            "Batch 240 | Loss: 0.8375\n",
            "Batch 250 | Loss: 0.9218\n",
            "Batch 260 | Loss: 0.7934\n",
            "Batch 270 | Loss: 0.7671\n",
            "Batch 280 | Loss: 0.7804\n",
            "Batch 290 | Loss: 0.7056\n",
            "Batch 300 | Loss: 1.1643\n",
            "Batch 310 | Loss: 0.7565\n",
            "Batch 320 | Loss: 0.7898\n",
            "Batch 330 | Loss: 0.7122\n",
            "Batch 340 | Loss: 0.7672\n",
            "Batch 350 | Loss: 0.7912\n",
            "Batch 360 | Loss: 1.1387\n",
            "Batch 370 | Loss: 0.8800\n",
            "Batch 380 | Loss: 0.6889\n",
            "Batch 390 | Loss: 0.7911\n",
            "Batch 400 | Loss: 0.7013\n",
            "‚úÖ Epoch 3 | Avg Loss: 0.8418\n",
            "Batch 0 | Loss: 0.7441\n",
            "Batch 10 | Loss: 0.5745\n",
            "Batch 20 | Loss: 0.8520\n",
            "Batch 30 | Loss: 0.7067\n",
            "Batch 40 | Loss: 0.6871\n",
            "Batch 50 | Loss: 1.3019\n",
            "Batch 60 | Loss: 1.3124\n",
            "Batch 70 | Loss: 0.8650\n",
            "Batch 80 | Loss: 0.8303\n",
            "Batch 90 | Loss: 0.9836\n",
            "Batch 100 | Loss: 0.9644\n",
            "Batch 110 | Loss: 0.6533\n",
            "Batch 120 | Loss: 0.5174\n",
            "Batch 130 | Loss: 0.5970\n",
            "Batch 140 | Loss: 0.7879\n",
            "Batch 150 | Loss: 0.5892\n",
            "Batch 160 | Loss: 0.4813\n",
            "Batch 170 | Loss: 1.1433\n",
            "Batch 180 | Loss: 0.7094\n",
            "Batch 190 | Loss: 0.5935\n",
            "Batch 200 | Loss: 0.8834\n",
            "Batch 210 | Loss: 0.7378\n",
            "Batch 220 | Loss: 1.0878\n",
            "Batch 230 | Loss: 0.5484\n",
            "Batch 240 | Loss: 0.6377\n",
            "Batch 250 | Loss: 0.4243\n",
            "Batch 260 | Loss: 1.0077\n",
            "Batch 270 | Loss: 1.1106\n",
            "Batch 280 | Loss: 0.5690\n",
            "Batch 290 | Loss: 0.4520\n",
            "Batch 300 | Loss: 0.5686\n",
            "Batch 310 | Loss: 0.4098\n",
            "Batch 320 | Loss: 0.7050\n",
            "Batch 330 | Loss: 0.4241\n",
            "Batch 340 | Loss: 0.6325\n",
            "Batch 350 | Loss: 0.6021\n",
            "Batch 360 | Loss: 0.4820\n",
            "Batch 370 | Loss: 0.4216\n",
            "Batch 380 | Loss: 0.5298\n",
            "Batch 390 | Loss: 0.9410\n",
            "Batch 400 | Loss: 0.6032\n",
            "‚úÖ Epoch 4 | Avg Loss: 0.6805\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Wav2Vec2ForCTC' object has no attribute 'unfreeze_feature_encoder'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2567892065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze_feature_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîì Feature encoder unfrozen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Wav2Vec2ForCTC' object has no attribute 'unfreeze_feature_encoder'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scoring engine"
      ],
      "metadata": {
        "id": "FGh5DorTdbgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attemp #1"
      ],
      "metadata": {
        "id": "SdaRdNvJFQ8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate_pronunciation(wav_path, target_phonemes):\n",
        "#     model.eval()\n",
        "#     wav, sr = torchaudio.load(wav_path)\n",
        "#     if sr != 16000:\n",
        "#         wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "\n",
        "#     input_values = processor(wav.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(input_values).logits\n",
        "#         probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "#     phoneme_results = []\n",
        "#     for p in target_phonemes:\n",
        "#         p_id = training_vocab.get(p)\n",
        "#         if p_id is not None:\n",
        "#             # We look for the HIGHEST confidence the model had for this sound\n",
        "#             # anywhere in the audio clip.\n",
        "#             p_score = torch.max(probs[0, :, p_id]).item()\n",
        "\n",
        "#             # Boost logic: If the model is 70% sure, for a child, that's an A!\n",
        "#             # We scale the score slightly to be more encouraging.\n",
        "#             adjusted_score = min(1.0, p_score * 1.2)\n",
        "\n",
        "#             phoneme_results.append({\n",
        "#                 \"phoneme\": p,\n",
        "#                 \"score\": adjusted_score,\n",
        "#                 \"feedback\": get_sinhala_feedback(adjusted_score)\n",
        "#             })\n",
        "#     return phoneme_results"
      ],
      "metadata": {
        "id": "OUf_jVPWdc_9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt #2"
      ],
      "metadata": {
        "id": "G5R3BLg4FTZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio.functional as F\n",
        "\n",
        "def evaluate_pronunciation(wav_path, target_phonemes):\n",
        "    model.eval()\n",
        "\n",
        "    wav, sr = torchaudio.load(wav_path)\n",
        "    if sr != 16000:\n",
        "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
        "\n",
        "    inputs = processor(\n",
        "        wav.squeeze().numpy(),\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emissions = model(inputs).logits.log_softmax(-1)\n",
        "\n",
        "    target_ids = torch.tensor(\n",
        "        [training_vocab[p] for p in target_phonemes],\n",
        "        device=device\n",
        "    ).unsqueeze(0)\n",
        "\n",
        "    alignment, scores = F.forced_align(\n",
        "        emissions,\n",
        "        target_ids,\n",
        "        blank=processor.tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for i, p in enumerate(target_phonemes):\n",
        "        raw_gop = scores[0][i].item()\n",
        "        confidence = normalize_gop(raw_gop)\n",
        "\n",
        "        results.append({\n",
        "            \"phoneme\": p,\n",
        "            \"score\": confidence,\n",
        "            \"feedback\": get_sinhala_feedback(confidence)\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "q_62_Ahjxe_5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize GOP\n",
        "\n",
        "import math\n",
        "\n",
        "def normalize_gop(log_prob, min_lp=-15.0, max_lp=-2.0):\n",
        "    \"\"\"\n",
        "    Maps log-probability to [0,1] confidence\n",
        "    \"\"\"\n",
        "    log_prob = max(min(log_prob, max_lp), min_lp)\n",
        "    return (log_prob - min_lp) / (max_lp - min_lp)\n"
      ],
      "metadata": {
        "id": "GpFEl3AMCK4d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sinhala Feedback"
      ],
      "metadata": {
        "id": "3n5diurYdobP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sinhala_feedback(score):\n",
        "    if score >= 0.85:\n",
        "        return \"‡∂â‡∂≠‡∑è ‡∑Ä‡∑í‡∑Å‡∑í‡∑Ç‡∑ä‡∂ß‡∂∫‡∑í! ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∂â‡∂≠‡∑è ‡∂±‡∑í‡∑Ä‡∑ê‡∂ª‡∂Ø‡∑í‡∑Ä ‡∂ã‡∂†‡∑ä‡∂†‡∑è‡∂ª‡∂´‡∂∫ ‡∂ö‡∑Ö‡∑è. üåü\" # Excellent\n",
        "    elif score >= 0.65:\n",
        "        return \"‡∑Ñ‡∑ú‡∂≥‡∂∫‡∑í, ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂≠‡∑Ä ‡∂ß‡∑í‡∂ö‡∂ö‡∑ä ‡∂¥‡∑î‡∑Ñ‡∑î‡∂´‡∑î ‡∑Ä‡∑ô‡∂∏‡∑î. ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∂≠‡∑Ä ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í ‡∂ö‡∂ª‡∂±‡∑ä‡∂± ‡∂ã‡∂≠‡∑ä‡∑É‡∑è‡∑Ñ ‡∂ö‡∂ª‡∂±‡∑ä‡∂±. üëç\" # Good, try more\n",
        "    elif score >= 0.40:\n",
        "        return \"‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂ã‡∂≠‡∑ä‡∑É‡∑è‡∑Ñ ‡∂ö‡∂ª‡∂∏‡∑î. ‡∂Ø‡∑í‡∑Ä ‡∑É‡∑Ñ ‡∂≠‡∑ú‡∂Ω‡∑ä ‡∂¥‡∑í‡∑Ñ‡∑í‡∂ß‡∑ì‡∂∏ ‡∂ú‡∑ê‡∂± ‡∑É‡∑ê‡∂Ω‡∂ö‡∑í‡∂Ω‡∑í‡∂∏‡∂≠‡∑ä ‡∑Ä‡∂±‡∑ä‡∂±. üëÇ\" # Try again, watch tongue/lips\n",
        "    else:\n",
        "        return \"‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\" # Hard to recognize"
      ],
      "metadata": {
        "id": "faU7qDWtdpk9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "tPFy_gEjd0sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_file = \"/content/drive/MyDrive/sinhala_pronunciation/dataset/balla/sound-72.wav\"\n",
        "test_targets = [\"b\", \"a\", \"l\", \"l\", \"a:\"]\n",
        "\n",
        "results = evaluate_pronunciation(test_file, test_targets)\n",
        "\n",
        "print(f\"--- ‡∂ã‡∂†‡∑ä‡∂†‡∑è‡∂ª‡∂´ ‡∑Ä‡∑è‡∂ª‡∑ä‡∂≠‡∑è‡∑Ä (Pronunciation Report) ---\")\n",
        "for r in results:\n",
        "    print(f\"Phoneme: {r['phoneme']} | Score: {r['score']:.2f}\")\n",
        "    print(f\"Feedback: {r['feedback']}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P88tPCsdsJd",
        "outputId": "96e7f8ff-5467-4a75-cbd0-45e906ed9243"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ‡∂ã‡∂†‡∑ä‡∂†‡∑è‡∂ª‡∂´ ‡∑Ä‡∑è‡∂ª‡∑ä‡∂≠‡∑è‡∑Ä (Pronunciation Report) ---\n",
            "Phoneme: b | Score: 0.23\n",
            "Feedback: ‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\n",
            "------------------------------\n",
            "Phoneme: a | Score: 0.23\n",
            "Feedback: ‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\n",
            "------------------------------\n",
            "Phoneme: l | Score: 0.28\n",
            "Feedback: ‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\n",
            "------------------------------\n",
            "Phoneme: l | Score: 0.31\n",
            "Feedback: ‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\n",
            "------------------------------\n",
            "Phoneme: a: | Score: 0.32\n",
            "Feedback: ‡∂ö‡∂´‡∂ú‡∑è‡∂ß‡∑î‡∂∫‡∑í, ‡∑Å‡∂∂‡∑ä‡∂Ø‡∂∫ ‡∑Ñ‡∂≥‡∑î‡∂±‡∑è ‡∂ú‡∑ê‡∂±‡∑ì‡∂∏‡∂ß ‡∂Ö‡∂¥‡∑Ñ‡∑É‡∑î‡∂∫‡∑í. ‡∂±‡∑ê‡∑Ä‡∂≠ ‡∂¥‡∑ê‡∑Ñ‡∑ê‡∂Ø‡∑í‡∂Ω‡∑í‡∑Ä ‡∂¥‡∑Ä‡∑É‡∂±‡∑ä‡∂±. ‚ùå\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3860509179.py:24: UserWarning: torchaudio.functional._alignment.forced_align has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  alignment, scores = F.forced_align(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rU_eDFMkeS01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}